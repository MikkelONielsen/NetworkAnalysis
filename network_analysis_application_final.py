# -*- coding: utf-8 -*-
"""Network_Analysis_Application_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10G1eeWlSMd3SUqDywNlNHuuIrJrsTet3

# **Network Analysis Application**

**1. Dataset Selection:**
  * We have choosen Twitter Interaction Network for the US Congress and obtained the dataset from the Stanford Large Network Dataset Collection
  * With this dataset we wish to make an application that is able to tell how influencial different people on twitter are on their network
  * First order of business is to install and import relevant libaries for the assignment
  * Next we obtain the dataset and preprocess it to start the analysis
"""

from community import community_louvain

# Packaging
import networkx as nx
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import altair as alt

# For visualization
!pip install -U bokeh
!pip install -q holoviews
!pip install -q datashader

sns.set()

# Import the libraries and link to the bokeh backend
import holoviews as hv
from holoviews import opts
hv.extension('bokeh')
from bokeh.plotting import show

# Setting the default figure size a bit larger
defaults = dict(width=750, height=750, padding=0.1,
                xaxis=None, yaxis=None)
hv.opts.defaults(
    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))

#importing the dataset and showing the head of the table
df = pd.read_json('https://raw.githubusercontent.com/MikkelONielsen/NetworkAnalysis/main/congress_network_data.json')
df

# Extract the necessary data from the dataframe
inList = df['inList'][0]
inWeight = df['inWeight'][0]
outList = df['outList'][0]
outWeight = df['outWeight'][0]
usernameList = df['usernameList'][0]

# Creating empty lists to hold the source, target, and weight values
sources = []
targets = []
weights = []

# Populating the lists with values from inList and outList
for i, (in_edges, out_edges) in enumerate(zip(inList, outList)):
    source = usernameList[i]

    # Handling inbound connections
    for target_index, weight in zip(in_edges, inWeight[i]):
        target = usernameList[target_index]
        sources.append(target)
        targets.append(source)
        weights.append(weight)

    # Handling outbound connections
    for target_index, weight in zip(out_edges, outWeight[i]):
        target = usernameList[target_index]
        sources.append(source)
        targets.append(target)
        weights.append(weight)

# Creating a DataFrame from the populated lists
network_df = pd.DataFrame({
    'source': sources,
    'target': targets,
    'weight': weights
})

# Display the first few rows of the new DataFrame to verify the structure
network_df.head()

"""From the original article we found information on the twitter-user's democratic party : https://pressgallery.house.gov/member-data/members-official-twitter-handles

**2. Graph Creation and Preprocessing:**
  * Take necessary steps to obtain a 1-mode graph of a reasonable size.
  * This might include node/edge filtering, graph projection, etc.

To filter the graph by weight and limit the number of nodes, we will follow these steps:

Filter by Weight: Determine an appropriate weight threshold and remove edges below this threshold.

Limit the Number of Nodes: Choose a method to select a subset of nodes. This can be done by:

Selecting the top nodes with the highest degree (most connections).

Selecting the nodes with the highest cumulative weight (most "influence" in the network).
"""

# Analyze the distribution of weights
plt.figure(figsize=(10, 6))
plt.hist(network_df['weight'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of Weights')
plt.xlabel('Weight')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate basic statistics to understand the distribution
weight_stats = network_df['weight'].describe()
weight_stats

"""The mean weight is approximately 0.0058.

The median weight (50th percentile) is about 0.0037.

The maximum weight is approximately 0.1306.

A reasonable weight cutoff might be at the 10 %. This would mean we only consider connections that are stronger than 90% of all connections.
"""

# Determine the weight threshold for the top 10%
top_10_percent_weight_threshold = network_df['weight'].quantile(0.9)

# Filter the edges above this new weight threshold
edges_top_10_percent_weight = network_df[network_df['weight'] > top_10_percent_weight_threshold]

# Now select the top 10% nodes based on degree
# Calculate the degree for each node
degree = edges_top_10_percent_weight['source'].append(edges_top_10_percent_weight['target']).value_counts()

# Determine the cutoff for the top 10% of nodes by degree
top_10_percent_nodes_cutoff = degree.quantile(0.9)

# Select the top 10% nodes
top_nodes_by_degree = degree[degree > top_10_percent_nodes_cutoff].index.tolist()

# Filter the DataFrame to include only edges that have both source and target in the top 10% nodes
filtered_network_df = edges_top_10_percent_weight[
    edges_top_10_percent_weight['source'].isin(top_nodes_by_degree) &
    edges_top_10_percent_weight['target'].isin(top_nodes_by_degree)
]

top_10_percent_weight_threshold, top_10_percent_nodes_cutoff, filtered_network_df.head(), len(filtered_network_df)

"""The new weight threshold for the top 10% of weights is approximately 0.0113. Additionally, nodes with more than 24 connections (top 10% by degree) were selected.

After applying these filters, we have 210 edges remaining in our filtered network. These edges represent the strongest connections between the most connected nodes in the network.
"""

# Create a graph from the filtered network dataframe
G = nx.from_pandas_edgelist(filtered_network_df, 'source', 'target', ['weight'])

# Draw the graph
plt.figure(figsize=(10, 10))
pos = nx.spring_layout(G, k=0.15, iterations=20)
nx.draw_networkx(G, pos, with_labels=True, node_size=50, font_size=10, edge_color='grey', linewidths=1)
plt.title('Top 10% Weighted 1-Mode Graph')
plt.axis('off')
plt.show()

"""**3. Node and Edge Characteristics:**

  * Node Importance: Calculate selected measures of node importance. Identify key nodes.
  * Edge Formation: If relevant, investigate drivers of edge creation, such as assortative mixing or the effects of different edge types in multidimensional networks.
"""

# Calculate the assortative coefficient for the network
# This number explains the relationship between the most influential users and the least influential. To explain this
# a negative number means that the influential people gets tweeted at lot by the lesser influential people and vice versa
assortativity_coefficient = nx.degree_assortativity_coefficient(G)
assortativity_coefficient

# Create a graph from the entire network dataframe
G_full = nx.from_pandas_edgelist(network_df, 'source', 'target', ['weight'])

# Calculate Degree Centrality
degree_centrality = nx.degree_centrality(G_full)

# Calculate Betweenness Centrality
betweenness_centrality = nx.betweenness_centrality(G_full)

# Calculate Eigenvector Centrality
eigenvector_centrality = nx.eigenvector_centrality_numpy(G_full)

# Combine all centrality measures into a single DataFrame for analysis
centrality_measures_df = pd.DataFrame({
    'Degree Centrality': degree_centrality,
    'Betweenness Centrality': betweenness_centrality,
    'Eigenvector Centrality': eigenvector_centrality
})

centrality_measures_df.sort_values(by=['Degree Centrality'], ascending=False).head()

centrality_measures_df.sort_values(by=['Betweenness Centrality'], ascending=False).head()

centrality_measures_df.sort_values(by=['Eigenvector Centrality'], ascending=False).head()

"""The calculated centrality measures for the nodes in the filtered graph provide insights into the node importance

**4. Community Detection:**
  * Use an appropriate community detection algorithm to detect partitions in the graph.
  * Characterize them in suitable ways, like summarizing attributes such as density, clustering coefficient, number of nodes, node characteristics, key nodes, etc.
"""

partition = community_louvain.best_partition(G_full)

# Create a new attribute in the graph for community membership
for node, community in partition.items():
    G_full.nodes[node]['community'] = community

# Function to characterize communities
def characterize_community(G, community_nodes):
    subgraph = G_full.subgraph(community_nodes)
    density = nx.density(subgraph)
    clustering_coefficient = nx.average_clustering(subgraph)
    avg_degree = sum(dict(subgraph.degree()).values()) / subgraph.number_of_nodes()
    return density, clustering_coefficient, avg_degree

# Apply the function to each community
community_characteristics = {}
unique_communities = set(partition.values())
for community in unique_communities:
    community_nodes = [node for node, comm in partition.items() if comm == community]
    community_characteristics[community] = characterize_community(G, community_nodes)

community_characteristics

"""**5. Data Visualization:**
  * Offer multiple visualizations of the graph, highlighting aspects like partitions, key nodes, density, etc.
  * Experiment with various types of network visualization. Provide insights and reasoning for your visualizations.
"""

# Function to visualize communities
def visualize_communities(G_full, partition):
    plt.figure(figsize=(10, 10))
    pos = nx.spring_layout(G_full)  # you can use other layout algorithms as well
    cmap = plt.get_cmap('viridis')  # choosing a colormap
    colors = [partition[node] for node in G_full.nodes]  # assigning colors based on community

    nx.draw_networkx_nodes(G_full, pos, node_color=colors, cmap=cmap)
    nx.draw_networkx_edges(G_full, pos, alpha=0.2)
    plt.show()

# Apply the function to visualize the graph
visualize_communities(G_full, partition)

"""The graph shows some clustering of communities, however theres way too many nodes and edges to see whats going on. Therefore we try to reduce the maximum nuber of nodes to 200 and use the 200 nodes that has the most edges.

Furthermore, we will use another more fancy visual that can also show more information when hovering over the nodes
"""

# Assuming 'partition' is a dictionary where keys are node names and values are community labels
# Add partition as a node attribute
for node, community in partition.items():
    G_full.nodes[node]['community'] = community

# Sorting nodes by degree centrality
sorted_nodes = sorted(nx.degree_centrality(G_full).items(), key=lambda x: x[1], reverse=True)

# Keeping the top 200 nodes
top_200_nodes = [node for node, centrality in sorted_nodes[:200]]

# Create a subgraph with the top 100 nodes
G_sub = G_full.subgraph(top_200_nodes)

# Create a layout for our nodes
layout = nx.layout.spring_layout(G_sub)

# Draw the network
graph = hv.Graph.from_networkx(G_sub, layout).opts(tools=['hover'], node_color='community')
labels = hv.Labels(graph.nodes, ['x', 'y'], 'index')

# Using datashader to handle larger graphs
from holoviews.operation.datashader import datashade, bundle_graph
bundled = bundle_graph(graph)

# Display the graph
show(hv.render(bundled * labels.opts(text_font_size='6pt', text_color='white', bgcolor='gray')))

